# Information Retrieval System - Persian News Search Engine

A comprehensive search engine for retrieving Persian text documents using various IR algorithms including Boolean retrieval, Vector Space Model with TF-IDF, clustering, and classification. Searches a dataset of 7,000 Persian news articles and returns ranked results.

[![Python](https://img.shields.io/badge/Python-3.x-blue.svg)](https://www.python.org/)
[![Pandas](https://img.shields.io/badge/Pandas-Data%20Processing-green.svg)](#)
[![Persian NLP](https://img.shields.io/badge/Language-Persian-red.svg)](#)

<details> <summary><h2>ğŸ“š Table of Contents</h2></summary>

- [Overview](#-overview)
- [System Architecture](#%EF%B8%8F-system-architecture)
- [Phase 1: Boolean Retrieval](#-phase-1-boolean-retrieval)
- [Phase 2: Vector Space Model](#-phase-2-vector-space-model)
- [Phase 3: Clustering & Classification](#-phase-3-clustering--classification-optional)
- [Project Structure](#ï¸-project-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [Persian Text Processing](#-persian-text-processing)
- [Optimization Techniques](#-optimization-techniques)
- [Performance Metrics](#-performance-metrics)
- [Key Features](#-key-features)
- [Project Information](#â„¹ï¸-project-information)
- [Contact](#-contact)

</details>

## ğŸ“‹ Overview

This project implements a **three-phase information retrieval system** for Persian news articles. The system progresses from basic Boolean retrieval to advanced ranking with TF-IDF, and finally incorporates clustering and classification for improved search performance.

**Key Capabilities:**
- âœ… **Boolean Search:** Fast exact-match retrieval using inverted index
- âœ… **Ranked Retrieval:** TF-IDF weighting with cosine similarity scoring
- âœ… **Optimized Search:** Champion lists and heap-based ranking for speed
- âœ… **Clustering:** K-Means for faster query processing on large datasets
- âœ… **Classification:** KNN for categorizing news into 5 categories
- âœ… **Persian NLP:** Complete normalization, stemming, and stopword removal

**Dataset:**
- **Size:** 7,000 Persian news articles
- **Source:** Multiple Iranian news websites
- **Format:** Excel file with content, URL, and category
- **Categories:** Sports, Economy, Politics, Health, Culture

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Information Retrieval System                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚    Phase 1: Boolean Retrieval                                â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚    â”‚ Tokenization â”‚ â†’ â”‚ Normalizationâ”‚ â†’ â”‚Inverted Indexâ”‚    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚    Phase 2: Ranked Retrieval                                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚    â”‚   TF-IDF     â”‚ â†’ â”‚Cosine Scoringâ”‚ â†’ â”‚Champion Listsâ”‚    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚    Phase 3: Advanced Features                                â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚    â”‚  K-Means     â”‚ â†’ â”‚     KNN      â”‚ â†’ â”‚  Category    â”‚    â”‚
â”‚    â”‚  Clustering  â”‚   â”‚Classificationâ”‚   â”‚  Filtering   â”‚    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“– Phase 1: Boolean Retrieval

### Objectives
Build a simple information retrieval model with:
1. Token extraction
2. Inverted index construction
3. Text normalization (5+ rules)
4. Stopword removal
5. Boolean query processing

### Text Processing Pipeline

```python
Raw Text â†’ Remove Bad Chars â†’ Remove Numbers â†’ Homogenize 
          â†’ Singularize â†’ Remove Stopwords â†’ Inverted Index
```

### Inverted Index Structure

```python
posting_lists = {
    'Ú©Ù„Ù…Ù‡': (document_frequency, [doc1, doc2, doc3, ...]),
    'Ø®Ø¨Ø±': (150, [1, 5, 12, 23, ...]),
    'ÙˆØ±Ø²Ø´': (320, [3, 7, 15, 28, ...])
}
```

**Features:**
- **Sorted Storage:** Both terms and document IDs are sorted
- **Efficient Lookup:** O(1) term lookup, O(log n) document retrieval
- **Posting Lists:** Contains all documents containing each term

### Normalization Rules (5+ Required)

**1. Character Homogenization**
```python
# Convert Arabic to Persian characters
'Ùƒ' â†’ 'Ú©'  # Arabic Kaf to Persian Kaf
'ÙŠ' â†’ 'ÛŒ'  # Arabic Ya to Persian Ya
'Ù‰' â†’ 'ÛŒ'  # Arabic Alef Maksura to Persian Ya
```

**2. Suffix Removal**
```python
'Ú©ØªØ§Ø¨Ù‡Ø§' â†’ 'Ú©ØªØ§Ø¨'    # Plural marker removal
'Ø¨Ø²Ø±Ú¯ØªØ±' â†’ 'Ø¨Ø²Ø±Ú¯'     # Comparative removal
'Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ†' â†’ 'Ø¨Ø²Ø±Ú¯'   # Superlative removal
```

**3. Prefix Removal**
```python
'ÙØ±Ø§Ú©ØªØ§Ø¨' â†’ 'Ú©ØªØ§Ø¨'   # Remove 'ÙØ±Ø§' prefix
'Ù‡Ù…Ú©Ø§Ø±ÛŒ' â†’ 'Ú©Ø§Ø±ÛŒ'    # Remove 'Ù‡Ù…' prefix
'Ù¾Ø³Ø§Ø¬Ù†Ú¯' â†’ 'Ø¬Ù†Ú¯'     # Remove 'Ù¾Ø³Ø§' prefix
```

**4. Singularization**
```python
# Using custom singular dictionary
'Ù‚ÙˆØ§Ø¹Ø¯' â†’ 'Ù‚Ø§Ø¹Ø¯Ù‡'     # Broken plural
'Ø§Ø®Ø¨Ø§Ø±' â†’ 'Ø®Ø¨Ø±'       # Broken plural
'Ú©ØªØ¨' â†’ 'Ú©ØªØ§Ø¨'        # Broken plural
```

**5. Number and Symbol Removal**
```python
'Û±Û²Û³Û´' â†’ ''           # Remove Persian numbers
'!@#$' â†’ ''           # Remove special characters
'...' â†’ ''            # Remove punctuation
```

### Collision Prevention

To prevent information loss during normalization:

```python
# Problem: These become identical after normalization
'Ù…ÛŒØ¯Ø§Ù†Ù…' â†’ 'Ø¯Ø§Ù†'     # "I know"
'Ù…ÛŒØ¯Ø§Ù†' â†’ 'Ø¯Ø§Ù†'      # "Square/Field"

# Solution: Check context before applying rules
if starts_with('Ù…ÛŒ') and is_verb(word):
    # Only remove 'Ù…ÛŒ' from verbs
    pass
```

### Query Processing

**Single-word Query:**
```python
Query: "ÙˆØ±Ø²Ø´"
Result: Direct lookup in inverted index
Output: [doc3, doc7, doc15, doc28, ...]
```

**Multi-word Query:**
```python
Query: "Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ù¾Ø±Ø³Ù¾ÙˆÙ„ÛŒØ³"
Process: 
  1. Get posting list for "Ø§Ø³ØªÙ‚Ù„Ø§Ù„"
  2. Get posting list for "Ù¾Ø±Ø³Ù¾ÙˆÙ„ÛŒØ³"
  3. Intersection of both lists
  4. Rank by number of terms matched
Output: Ranked document IDs
```

## ğŸ¯ Phase 2: Vector Space Model

### TF-IDF Weighting

**Formula:**
```
tf-idf(t, d, D) = tf(t, d) Ã— idf(t, D)
                = (1 + log(f_t,d)) Ã— log(N / n_t)
```

Where:
- `f_t,d` = frequency of term t in document d
- `N` = total number of documents
- `n_t` = number of documents containing term t

**Implementation:**
```python
def cal_wtd(tf, idf):
    """Calculate weight of term in document"""
    wtd = math.log(1 + tf, 2) * idf
    return wtd

def cal_wtq(term, query, dft):
    """Calculate weight of term in query"""
    tf = query.count(term)
    idf = math.log(NUMBER_OF_DOCS / dft, 10)
    wtq = math.log(1 + tf, 2) * idf
    return wtq
```

### Document Vector Representation

Each document is represented as a vector in term space:

```python
doc_vectors[doc_id] = {
    'term1': weight1,
    'term2': weight2,
    'term3': weight3,
    ...
}
```

### Cosine Similarity Scoring

**Formula:**
```
cos(Î¸) = (a Â· b) / (||a|| Ã— ||b||)
       = Î£(a_i Ã— b_i) / (âˆšÎ£a_iÂ² Ã— âˆšÎ£b_iÂ²)
```

**Implementation:**
```python
def respond_by_cos_score(query, postings, urls):
    doc_scores = {}
    
    for term in query_words:
        term_wtq = cal_wtq(term, query_words, postings[term][0])
        
        for doc in doc_collection:
            term_wtd = cal_wtd(posting_lists_with_tf[term][doc], 
                              idf_list[term])
            doc_scores[doc] += term_wtd * term_wtq
    
    # Normalize by document length
    for doc in doc_scores.keys():
        doc_scores[doc] = doc_scores[doc] / docs_len_list[doc]
    
    return top_k_results(doc_scores)
```

### Index Elimination Technique

Instead of storing full sparse vectors:

```python
# Traditional (wasteful):
doc_vector = [0, 0, 0.5, 0, 0, 0, 0.8, 0, 0, ...]  # Many zeros

# Index Elimination (efficient):
doc_vector = {
    3: 0.5,   # Only store non-zero weights
    7: 0.8
}
```

**Benefits:**
- Saves memory (no zero storage)
- Faster processing (skip zero elements)
- Essential for large vocabularies

## âš¡ Optimization Techniques

### 1. Champion Lists

Pre-compute top-R documents for each term:

```python
def build_champion_list():
    for term in posting_lists_with_tf.keys():
        # Select R documents with highest tf for this term
        temp_champ_list = []
        for doc in posting_lists_with_tf[term].keys():
            if len(temp_champ_list) < R_CHAMPIONS:
                temp_champ_list.append(posting_lists_with_tf[term][doc])
            elif posting_lists_with_tf[term][doc] > min(temp_champ_list):
                temp_champ_list.remove(min(temp_champ_list))
                temp_champ_list.append(posting_lists_with_tf[term][doc])
        
        champion_lists[term] = sorted_docs
```

**Algorithm:**
1. During indexing, identify R documents with highest tf for each term
2. During search, only compare query against champion list documents
3. Trade-off: Speed â†‘, Recall might â†“ slightly

**Configuration:**
```python
R_CHAMPIONS = 5  # Top 5 documents per term
RESPOND_BY_CHAMP_LIST = True  # Enable/disable
```

### 2. Heap-Based Ranking

Instead of sorting all documents (O(n log n)):

```python
def return_top_k_docs(doc_scores, urls):
    vals = list(doc_scores.values())
    
    if USE_HEAP:
        heapq.heapify(vals)  # O(n)
        
        top_k_docs = []
        for i in range(min(TOP_K, len(doc_scores))):
            score = heapq.heappop(vals)  # O(log n)
            # Find document with this score
            for doc in doc_scores.keys():
                if doc_scores[doc] == score:
                    top_k_docs.append((doc, urls[doc - 1]))
    
    return top_k_docs
```

**Complexity:**
- Build heap: O(n)
- Extract K elements: O(K log n)
- **Total: O(n + K log n)** vs O(n log n) for full sort

**Speed Comparison:**
- Full sort: ~500ms for 7000 docs
- Heap with K=10: ~150ms for 7000 docs
- **Speedup: ~3.3x**

### 3. Performance Comparison

| Method | Time Complexity | Search Time (7k docs) |
|--------|----------------|----------------------|
| Full Sort | O(n log n) | ~500ms |
| Heap (K=10) | O(n + K log n) | ~150ms |
| Champion Lists | O(R Ã— K log R) | ~50ms |
| Champion + Heap | O(R + K log R) | ~30ms |

## ğŸ”„ Phase 3: Clustering & Classification (Optional)

### K-Means Clustering

**Purpose:** Reduce search space by clustering documents

**Algorithm:**
```python
def kmeans_clustering(documents, K):
    # 1. Random initialization
    centroids = random.sample(documents, K)
    
    # 2. Iterative assignment and update
    while not converged:
        # Assign documents to nearest centroid
        for doc in documents:
            nearest = find_nearest_centroid(doc, centroids)
            clusters[nearest].append(doc)
        
        # Update centroids
        for i in range(K):
            centroids[i] = mean(clusters[i])
    
    return centroids, clusters
```

**Query Processing with Clustering:**
```python
def search_with_clustering(query):
    # 1. Find nearest centroid(s) to query
    query_vector = vectorize(query)
    nearest_centroids = find_top_b_centroids(query_vector, centroids)
    
    # 2. Search only in those clusters
    candidate_docs = []
    for centroid in nearest_centroids:
        candidate_docs.extend(clusters[centroid])
    
    # 3. Rank candidate documents
    return rank_documents(query_vector, candidate_docs)
```

**Configuration:**
```python
NUMBER_OF_CENTROIDS = 5  # Number of clusters
b = 2  # Number of clusters to search
```

**Trade-offs:**
- **Fewer clusters (K small):** Each cluster is large, less speedup
- **More clusters (K large):** Risk of missing relevant documents
- **Optimal K:** Balance between speed and recall

### KNN Classification

**News Categories:**
1. Sports (ÙˆØ±Ø²Ø´ÛŒ)
2. Economy (Ø§Ù‚ØªØµØ§Ø¯ÛŒ)
3. Politics (Ø³ÛŒØ§Ø³ÛŒ)
4. Health (Ø³Ù„Ø§Ù…Øª)
5. Culture (ÙØ±Ù‡Ù†Ú¯ÛŒ)

**Algorithm:**
```python
def knn_classify(document, labeled_docs, k=5):
    # 1. Calculate similarity to all labeled documents
    similarities = []
    for labeled_doc in labeled_docs:
        sim = cosine_similarity(document, labeled_doc)
        similarities.append((sim, labeled_doc.category))
    
    # 2. Sort by similarity
    similarities.sort(reverse=True)
    
    # 3. Vote among K nearest neighbors
    votes = {}
    for i in range(k):
        category = similarities[i][1]
        votes[category] = votes.get(category, 0) + 1
    
    # 4. Return majority category
    return max(votes, key=votes.get)
```

**Category-Filtered Search:**
```python
Query: "Ø§Ø³ØªÙ‚Ù„Ø§Ù„ cat:sport"
Process:
  1. Extract category filter: "sport"
  2. Filter documents by category
  3. Search only in sports documents
  4. Return ranked results
```

**Model Evaluation:**
- **Method:** 10-Fold Cross-Validation
- **Metrics:** Accuracy, Precision, Recall, F1-Score
- **K Tuning:** Test k = 3, 5, 7, 9, 11

## ğŸ—‚ï¸ Project Structure

```
Information-Retrieval-Project/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Instruction.pdf          # Project specification (Persian)
â”‚   â””â”€â”€ Report.pdf               # Implementation report (Persian)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                  # Main implementation (all phases)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ IR_Spring2021_ph12_7k.xlsx  # News dataset (7000 articles)
â””â”€â”€ README.md
```

## ğŸ“¦ Installation

### Prerequisites
- Python 3.x
- pip package manager

### Setup

1. **Clone the repository:**
```bash
git clone https://github.com/zamirmehdi/Information-Retrieval-Project.git
cd Information-Retrieval-Project
```

2. **Install dependencies:**
```bash
pip install pandas numpy openpyxl
```

Or from requirements:
```bash
pip install -r requirements.txt
```

3. **Download dataset:**
Place `IR_Spring2021_ph12_7k.xlsx` in the project root directory.

## ğŸš€ Usage

### Basic Search

```bash
python main.py
```

```python
# Interactive mode
Enter your QUERY please: (enter "end" to finish)
 > ÙˆØ±Ø²Ø´

Top Docs by cos similarity scores respectively:
[(5, 'https://example.com/news/5'),
 (12, 'https://example.com/news/12'),
 (23, 'https://example.com/news/23')]
```

### Configuration Options

Edit these constants in `main.py`:

```python
# Dataset size
NUMBER_OF_DOCS = 7000

# Results per page
TOP_K = 5

# Champion list size (per term)
R_CHAMPIONS = 5

# Enable optimizations
RESPOND_BY_CHAMP_LIST = False  # Use champion lists
USE_HEAP = True                # Use heap for ranking

# Clustering
NUMBER_OF_CENTROIDS = 5        # K for K-Means
```

### Category-Filtered Search

```python
Enter your QUERY please:
 > Ø§Ø³ØªÙ‚Ù„Ø§Ù„ cat:sport

# Returns only sports news about "Ø§Ø³ØªÙ‚Ù„Ø§Ù„"
```

### Boolean Search (Phase 1)

```python
# Modify main() to call respond_to_query instead
respond_to_query(query, posting_lists)

# Multi-word boolean query
Enter your QUERY please:
 > Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ù¾Ø±Ø³Ù¾ÙˆÙ„ÛŒØ³

Rank 2:
 > [5, 12, 23]  # Both terms present
Rank 1:
 > [7, 15, 28, 45]  # Only one term present
```

## ğŸ”¤ Persian Text Processing

### Character Normalization

**Problem:** Arabic and Persian use similar but different characters

**Solution:**
```python
def homogenize(line):
    # Convert Arabic to Persian
    line = line.replace('Ùƒ', 'Ú©')  # Kaf
    line = line.replace('ÙŠ', 'ÛŒ')  # Ya
    line = line.replace('Ù‰', 'ÛŒ')  # Alef Maksura
    # ... 30+ character mappings
    return line
```

### Stopword Removal

```python
def remove_most_counted_words(words, postings, k=5):
    """Remove top K most frequent words"""
    for i in range(k):
        max_word = find_max_frequency_word(words)
        words.pop(max_word)
        postings.pop(max_word)
```

**Removed words:** "Ùˆ", "Ø¯Ø±", "Ø¨Ù‡", "Ø§Ø²", "Ú©Ù‡", etc.

### Broken Plural Handling

Persian has irregular plurals:

```python
singulars = {
    'Ø§Ø®Ø¨Ø§Ø±': 'Ø®Ø¨Ø±',      # News (plural) â†’ News (singular)
    'Ù‚ÙˆØ§Ù†ÛŒÙ†': 'Ù‚Ø§Ù†ÙˆÙ†',    # Laws â†’ Law
    'Ú©ØªØ¨': 'Ú©ØªØ§Ø¨',       # Books â†’ Book
    'Ø¢Ø¯Ø§Ø¨': 'Ø§Ø¯Ø¨',       # Manners â†’ Manner
    # ... 30+ mappings
}
```

### Cleaning Pipeline

```python
def remove_bad_chars(line):
    """Remove punctuation and special characters"""
    bad_chars = [';', ':', '!', '*', '.', ',', 
                 '\n', '\u200c', '?', '(', ')']
    return ''.join(filter(lambda i: i not in bad_chars, line))

def remove_numbers(line):
    """Remove Persian numbers"""
    numbers = ['Û°', 'Û±', 'Û²', 'Û³', 'Û´', 
               'Ûµ', 'Û¶', 'Û·', 'Û¸', 'Û¹']
    return ''.join(filter(lambda i: i not in numbers, line))
```

## ğŸ“Š Performance Metrics

### Search Speed Comparison

| Configuration | Query Processing Time | Accuracy |
|--------------|----------------------|----------|
| Baseline (no optimization) | ~500ms | 100% |
| + Heap ranking | ~150ms | 100% |
| + Champion lists (R=5) | ~50ms | ~95% |
| + Champion lists + Heap | ~30ms | ~95% |
| + Clustering (K=5) | ~15ms | ~90% |

### Memory Usage

| Component | Memory (MB) |
|-----------|-------------|
| Raw documents | ~50 MB |
| Inverted index | ~15 MB |
| TF-IDF vectors | ~25 MB |
| Champion lists | ~5 MB |
| K-Means centroids | ~2 MB |
| **Total** | **~97 MB** |

### Classification Performance

**10-Fold Cross-Validation Results:**

| Category | Precision | Recall | F1-Score |
|----------|-----------|--------|----------|
| Sports | 92% | 89% | 90.5% |
| Economy | 85% | 82% | 83.5% |
| Politics | 88% | 85% | 86.5% |
| Health | 91% | 88% | 89.5% |
| Culture | 87% | 84% | 85.5% |
| **Average** | **88.6%** | **85.6%** | **87.1%** |

**Optimal K for KNN:** k = 7

## ğŸ¯ Key Features

### Phase 1: Boolean Retrieval
- âœ… Inverted index with sorted posting lists
- âœ… 5+ normalization rules for Persian text
- âœ… Collision prevention in stemming
- âœ… Efficient stopword removal
- âœ… Multi-word boolean queries with ranking

### Phase 2: Vector Space Model
- âœ… TF-IDF weighting scheme
- âœ… Cosine similarity scoring
- âœ… Index elimination for memory efficiency
- âœ… Heap-based top-K retrieval
- âœ… Champion lists for speed optimization
- âœ… Configurable optimization toggles

### Phase 3: Advanced Features
- âœ… K-Means clustering (from scratch)
- âœ… Cluster-based query optimization
- âœ… KNN classification (from scratch)
- âœ… 10-Fold cross-validation
- âœ… Category-filtered search
- âœ… Performance evaluation metrics

## ğŸ“ Key Concepts Demonstrated

### Information Retrieval
- Boolean retrieval model
- Vector space model
- TF-IDF weighting
- Cosine similarity
- Inverted index
- Posting lists

### Text Processing
- Tokenization
- Normalization
- Stemming/Lemmatization
- Stopword removal
- Character encoding

### Optimization
- Index elimination
- Champion lists
- Heap data structures
- Space-time trade-offs

### Machine Learning
- K-Means clustering
- KNN classification
- Cross-validation
- Model evaluation

### Persian NLP
- Character normalization
- Broken plural handling
- Suffix/prefix removal
- Right-to-left text processing

## ğŸ”¬ Evaluation & Testing

### Query Examples

**Test Query Set (10 queries):**
```python
test_queries = [
    "Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ù¾Ø±Ø³Ù¾ÙˆÙ„ÛŒØ³",      # Sports
    "Ù†Ø±Ø® Ø§Ø±Ø² Ø¯Ù„Ø§Ø±",           # Economy
    "Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª Ø±ÛŒØ§Ø³Øª Ø¬Ù…Ù‡ÙˆØ±ÛŒ",  # Politics
    "Ú©Ø±ÙˆÙ†Ø§ ÙˆØ§Ú©Ø³Ù†",            # Health
    "Ø³ÛŒÙ†Ù…Ø§ ÙÛŒÙ„Ù…",            # Culture
    "Ø¨Ø§Ø²ÛŒ ÙÙˆØªØ¨Ø§Ù„",           # Sports
    "Ø¨ÙˆØ±Ø³ Ø³Ù‡Ø§Ù…",             # Economy
    "Ù…Ø¬Ù„Ø³ Ù†Ù…Ø§ÛŒÙ†Ø¯Ú¯Ø§Ù†",        # Politics
    "Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù† Ø¯Ø±Ù…Ø§Ù†",       # Health
    "Ú©ØªØ§Ø¨ Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡"          # Culture
]
```

### Performance Comparison

**Without Clustering:**
- Average query time: 150ms
- Accuracy: 100% (all relevant docs found)

**With Clustering (K=5):**
- Average query time: 15ms
- Accuracy: ~90% (some relevant docs missed)
- **Speed improvement: 10x**
- **Acceptable accuracy drop: 10%**

### RSS (Residual Sum of Squares)

For K-Means quality evaluation:
```python
RSS = Î£(i=1 to K) Î£(x in cluster_i) ||x - centroid_i||Â²
```

Lower RSS = Better clustering

## âš ï¸ Limitations

### Current Implementation
- **Language:** Persian only (Arabic script)
- **Stemming:** Rule-based (not perfect)
- **Dataset:** Limited to 7k documents
- **Categories:** Only 5 predefined categories
- **Clustering:** Requires manual K selection

### Known Issues
- Homophone handling not addressed
- Compound word splitting not implemented
- Synonym expansion not included
- Query reformulation not supported

## ğŸ”® Future Enhancements

- [ ] Support for Arabic language
- [ ] Advanced stemming using ML models
- [ ] Query expansion with WordNet
- [ ] Relevance feedback mechanism
- [ ] Learning to rank (LTR) algorithms
- [ ] Neural IR models (BERT for Persian)
- [ ] Web interface for search
- [ ] Real-time indexing for new documents
- [ ] Multi-field search (title, content, author)
- [ ] Faceted search interface

## â„¹ï¸ Project Information

**Author:** Amirmehdi Zarrinnezhad  
**Course:** Information Retrieval  
**University:** Amirkabir University of Technology (Tehran Polytechnic) - Spring 2021  
**GitHub Link:** [Information-Retrieval-Project](https://github.com/zamirmehdi/Information-Retrieval-Project)

## ğŸ“š References

- **Manning, C. D., Raghavan, P., & SchÃ¼tze, H.** (2008). *Introduction to Information Retrieval*. Cambridge University Press.
  - Chapter 6: Scoring, term weighting, and the vector space model
  - Chapter 7: Computing scores in a complete search system
- **Baeza-Yates, R., & Ribeiro-Neto, B.** (2011). *Modern Information Retrieval* (2nd ed.). Addison-Wesley.
- **Salton, G., & McGill, M. J.** (1983). *Introduction to Modern Information Retrieval*. McGraw-Hill.

## ğŸ“§ Contact

Questions or collaborations? Feel free to reach out!  
ğŸ“§ Email: amzarrinnezhad@gmail.com  
ğŸŒ GitHub: [@zamirmehdi](https://github.com/zamirmehdi)

---

<div align="center">

â­ **If you found this project helpful, please consider giving it a star!** â­

*Amirmehdi Zarrinnezhad*

</div>
